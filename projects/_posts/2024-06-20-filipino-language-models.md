---
layout: page
title: Pre-trained Language Models for Filipino
date: 2024-06-20
description: Creating comprehensive pre-trained language models for Filipino languages to enable downstream NLP tasks and applications.
categories: projects
---

## Overview

This project focuses on developing robust pre-trained language models specifically designed for Filipino languages, leveraging large-scale web corpora and advanced training techniques.

## Objectives

- Collect and curate large-scale Filipino text corpora from diverse sources
- Train BERT, RoBERTa, and GPT-style models for Filipino
- Fine-tune models for various downstream tasks (NER, sentiment analysis, QA)
- Release models and tools to the research community

## Key Features

- Multi-dialect support (Tagalog, Cebuano, Ilocano, Hiligaynon)
- Domain-adaptive pre-training for specialized applications
- Efficient model architectures for resource-constrained deployment
- Comprehensive evaluation benchmarks

## Current Progress

We have successfully trained a RoBERTa-base model on 10GB of Filipino text and achieved state-of-the-art results on several downstream tasks. Currently expanding to larger model sizes and more language varieties.

## Team

This is a collaborative effort involving researchers from multiple institutions and industry partners.

## Links

- [Model Cards on HuggingFace](#)
- [Training Code on GitHub](#)
- [Benchmark Results](#)
