---
layout: project
title: Uy PilipINST! Creating an instruction-tuning dataset for Philippine languages
date: 2025-10-30
description: We plan to create a high-quality instruction-tuning dataset for the four widely spoken PH languages (Tagalog, Cebuano, Ilocano, Hiligaynon), with around 10k samples per language. 
categories: projects
expected_date_range: Feb 2026 - Jan 2027 
skills:
  - Instruction Tuning
  - Data Collection and Curation
  - Benchmarking
category: Dataset Curation
lead: Lj V. Miranda 
lead_email: ljvm2@cam.ac.uk
lead_website: https://ljvmiranda.github.io/
---


Instruction-tuning (or supervised fine-tuning, SFT) is a method to further refine a model's capabilities for specific tasks or languages.
It is relatively cheap and straightforward, which is why most foundation model providers include instruction-tuned models in their releases.

Since most foundation model providers rely on open-source datasets to train their models, we can indirectly influence their development pipelines by **contributing a high-quality instruction-tuning dataset to the open ecosystem**.
This effort also paves the way for training our own Filipino-centric language models.


## What exactly are we trying to do?
