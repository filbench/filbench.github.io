---
layout: project
title: Uy PilipINST! Creating an instruction-tuning dataset for Philippine languages
date: 2025-10-30
description: We plan to create a high-quality instruction-tuning dataset for the four widely spoken PH languages (Tagalog, Cebuano, Ilocano, Hiligaynon), with around 10k samples per language. 
categories: projects
expected_date_range: Feb 2026 - Jan 2027 
skills:
  - Instruction Tuning
  - Data Collection and Curation
  - Benchmarking
category: Dataset Curation
lead: Lj V. Miranda 
lead_email: ljvm2@cam.ac.uk
lead_website: https://ljvmiranda.github.io/
---


Instruction-tuning (or supervised finetuning, SFT) is a method to further refine a model's capabilities on a specific task or language. 
It is relatively cheap and simple, and most foundation model providers include instruction-tuned models in their releases.

Most foundation model providers rely on open-source datasets as a data source to train their models.
I believe that we can indirectly influence their model development pipelines by **contributing a high-quality instruction-tuning dataset in the open ecosystem**.
In addition, this effort also paves the way for training our own Filipino-centric language models.


## What exactly are we trying to do?
